{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea3b6f-d657-4b1f-9b9f-9f69e803c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import json\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "# from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler, LabelEncoder\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Optional imports\n",
    "# try:\n",
    "#     from imblearn.over_sampling import SMOTE\n",
    "#     from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "#     IMBLEARN_AVAILABLE = True\n",
    "# except Exception:\n",
    "#     IMBLEARN_AVAILABLE = False\n",
    "\n",
    "# try:\n",
    "#     from xgboost import XGBClassifier\n",
    "#     XGBOOST_AVAILABLE = True\n",
    "# except Exception:\n",
    "#     XGBOOST_AVAILABLE = False\n",
    "\n",
    "# def load_and_enhance_data(file_path):\n",
    "#     \"\"\"Load data and create enhanced features\"\"\"\n",
    "#     df = pd.read_csv('accident-project.csv')\n",
    "#     df.columns = df.columns.str.strip()\n",
    "    \n",
    "#     # Clean data\n",
    "#     df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    \n",
    "#     # Enhanced time features\n",
    "#     if 'Time' in df.columns:\n",
    "#         df['Time_parsed'] = pd.to_datetime(df['Time'], format='%H:%M:%S', errors='coerce')\n",
    "#         df.loc[df['Time_parsed'].isna(), 'Time_parsed'] = pd.to_datetime(\n",
    "#             df.loc[df['Time_parsed'].isna(), 'Time'], format='%H:%M', errors='coerce'\n",
    "#         )\n",
    "#         df['Hour_of_Day'] = df['Time_parsed'].dt.hour\n",
    "#         df['Time_of_Day'] = pd.cut(df['Hour_of_Day'], \n",
    "#                                  bins=[0, 6, 12, 18, 24], \n",
    "#                                  labels=['Night', 'Morning', 'Afternoon', 'Evening'],\n",
    "#                                  include_lowest=True)\n",
    "#         df = df.drop(columns=['Time', 'Time_parsed'], errors='ignore')\n",
    "    \n",
    "#     # Create accident severity risk score\n",
    "#     risk_factors = []\n",
    "#     if 'Number_of_vehicles_involved' in df.columns:\n",
    "#         risk_factors.append(df['Number_of_vehicles_involved'])\n",
    "#     if 'Number_of_casualties' in df.columns:\n",
    "#         risk_factors.append(df['Number_of_casualties'])\n",
    "    \n",
    "#     if risk_factors:\n",
    "#         df['Risk_Score'] = sum(risk_factors) / len(risk_factors)\n",
    "    \n",
    "#     # Enhanced driver experience categorization\n",
    "#     if 'Driving_experience' in df.columns:\n",
    "#         df['Experience_Level'] = df['Driving_experience'].map({\n",
    "#             'No Licence': 'Novice',\n",
    "#             'Below 1yr': 'Novice', \n",
    "#             '1-2yr': 'Beginner',\n",
    "#             '2-5yr': 'Intermediate',\n",
    "#             '5-10yr': 'Experienced',\n",
    "#             'Above 10yr': 'Expert',\n",
    "#             'Unknown': 'Unknown'\n",
    "#         })\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# def create_preprocessor(numeric_features, ordinal_cols, nominal_cols, ordinal_categories):\n",
    "#     \"\"\"Create preprocessing pipeline\"\"\"\n",
    "#     numeric_transformer = Pipeline([\n",
    "#         ('imputer', SimpleImputer(strategy='median')),\n",
    "#         ('scaler', StandardScaler())\n",
    "#     ])\n",
    "\n",
    "#     ordinal_transformer = Pipeline([\n",
    "#         ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#         ('encoder', OrdinalEncoder(categories=ordinal_categories, \n",
    "#                                   handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "#     ])\n",
    "\n",
    "#     nominal_transformer = Pipeline([\n",
    "#         ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#         ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "#     ])\n",
    "\n",
    "#     preprocessor = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             ('num', numeric_transformer, numeric_features),\n",
    "#             ('ord', ordinal_transformer, ordinal_cols),\n",
    "#             ('cat', nominal_transformer, nominal_cols)\n",
    "#         ],\n",
    "#         remainder='drop'\n",
    "#     )\n",
    "    \n",
    "#     return preprocessor\n",
    "\n",
    "# def train_models():\n",
    "#     \"\"\"Main training function\"\"\"\n",
    "#     print(\"üöÄ Starting Model Training...\")\n",
    "    \n",
    "#     # Load and prepare data\n",
    "#     file_path = \"accident-project.csv\"\n",
    "#     df = load_and_enhance_data(file_path)\n",
    "    \n",
    "#     # Drop duplicates and missing target\n",
    "#     df = df.drop_duplicates()\n",
    "#     target_col = \"Accident_severity\"\n",
    "#     df = df[~df[target_col].isna()].copy()\n",
    "    \n",
    "#     print(f\"üìä Dataset shape: {df.shape}\")\n",
    "#     print(f\"üéØ Target distribution:\\n{df[target_col].value_counts()}\")\n",
    "    \n",
    "#     # Feature groups\n",
    "#     numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "#     categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "#     # Remove target\n",
    "#     numeric_features = [c for c in numeric_features if c != target_col]\n",
    "#     categorical_features = [c for c in categorical_features if c != target_col]\n",
    "    \n",
    "#     # Add engineered features\n",
    "#     if 'Hour_of_Day' in df.columns and 'Hour_of_Day' not in numeric_features:\n",
    "#         numeric_features.append('Hour_of_Day')\n",
    "#     if 'Risk_Score' in df.columns and 'Risk_Score' not in numeric_features:\n",
    "#         numeric_features.append('Risk_Score')\n",
    "    \n",
    "#     # Ordinal features with proper ordering\n",
    "#     ordinal_mappings = {\n",
    "#         'Day_of_week': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "#         'Age_band_of_driver': ['Under 18', '18-30', '31-50', 'Over 51', 'Unknown'],\n",
    "#         'Educational_level': ['Illiterate', 'Writing & reading', 'Elementary school', \n",
    "#                              'Junior high school', 'High school', 'Above high school', 'Unknown'],\n",
    "#         'Driving_experience': ['No Licence', 'Below 1yr', '1-2yr', '2-5yr', '5-10yr', 'Above 10yr', 'Unknown'],\n",
    "#         'Service_year_of_vehicle': ['Below 1yr', '1-2yr', '2-5yr', '5-10yr', 'Above 10yr', 'Unknown'],\n",
    "#         'Experience_Level': ['Novice', 'Beginner', 'Intermediate', 'Experienced', 'Expert', 'Unknown']\n",
    "#     }\n",
    "    \n",
    "#     ordinal_cols = [col for col in ordinal_mappings.keys() if col in df.columns]\n",
    "#     ordinal_categories = [ordinal_mappings[col] for col in ordinal_cols]\n",
    "    \n",
    "#     # Nominal features\n",
    "#     nominal_cols = [c for c in categorical_features if c not in ordinal_cols]\n",
    "    \n",
    "#     print(f\"üîß Processing {len(numeric_features)} numeric, {len(ordinal_cols)} ordinal, {len(nominal_cols)} nominal features\")\n",
    "    \n",
    "#     # Create preprocessor\n",
    "#     preprocessor = create_preprocessor(numeric_features, ordinal_cols, nominal_cols, ordinal_categories)\n",
    "    \n",
    "#     # Prepare target\n",
    "#     le_target = LabelEncoder()\n",
    "#     y = le_target.fit_transform(df[target_col])\n",
    "#     X = df.drop(columns=[target_col])\n",
    "    \n",
    "#     target_mapping = dict(zip(le_target.classes_, le_target.transform(le_target.classes_)))\n",
    "#     print(\"üéØ Target mapping:\", target_mapping)\n",
    "    \n",
    "#     # Train-test split\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X, y, test_size=0.2, stratify=y, random_state=42\n",
    "#     )\n",
    "    \n",
    "#     # Define models\n",
    "#     models = {\n",
    "#         'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
    "#         'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "#         'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=100),\n",
    "#         'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "#         'SVM': SVC(random_state=42, class_weight='balanced', probability=True)\n",
    "#     }\n",
    "    \n",
    "#     if XGBOOST_AVAILABLE:\n",
    "#         models['XGBoost'] = XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "    \n",
    "#     # Train models\n",
    "#     best_models = {}\n",
    "#     results = {}\n",
    "    \n",
    "#     print(\"\\nüîÆ Training Models...\")\n",
    "#     for name, model in models.items():\n",
    "#         print(f\"Training {name}...\")\n",
    "        \n",
    "#         # Create pipeline\n",
    "#         if IMBLEARN_AVAILABLE:\n",
    "#             pipeline = ImbPipeline([\n",
    "#                 ('preprocessor', preprocessor),\n",
    "#                 ('smote', SMOTE(random_state=42)),\n",
    "#                 ('clf', model)\n",
    "#             ])\n",
    "#         else:\n",
    "#             pipeline = Pipeline([\n",
    "#                 ('preprocessor', preprocessor),\n",
    "#                 ('clf', model)\n",
    "#             ])\n",
    "        \n",
    "#         # Train model\n",
    "#         pipeline.fit(X_train, y_train)\n",
    "        \n",
    "#         # Evaluate\n",
    "#         y_pred = pipeline.predict(X_test)\n",
    "#         metrics = {\n",
    "#             'Accuracy': accuracy_score(y_test, y_pred),\n",
    "#             'Precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "#             'Recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "#             'F1-Score': f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "#         }\n",
    "        \n",
    "#         best_models[name] = pipeline\n",
    "#         results[name] = metrics\n",
    "        \n",
    "#         print(f\"  ‚úÖ {name}: F1 = {metrics['F1-Score']:.4f}\")\n",
    "    \n",
    "#     # Select best model\n",
    "#     best_model_name = max(results, key=lambda x: results[x]['F1-Score'])\n",
    "#     best_model = best_models[best_model_name]\n",
    "    \n",
    "#     print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "#     print(f\"üìä Performance: {results[best_model_name]}\")\n",
    "    \n",
    "#     # Save model artifacts\n",
    "#     model_artifacts = {\n",
    "#         'model': best_model,\n",
    "#         'preprocessor': preprocessor,\n",
    "#         'label_encoder': le_target,\n",
    "#         'target_mapping': target_mapping,\n",
    "#         'feature_names': {\n",
    "#             'numeric': numeric_features,\n",
    "#             'ordinal': ordinal_cols,\n",
    "#             'nominal': nominal_cols\n",
    "#         },\n",
    "#         'model_performance': results,\n",
    "#         'best_model_name': best_model_name,\n",
    "#         'all_models': best_models\n",
    "#     }\n",
    "    \n",
    "#     # Save with pickle\n",
    "#     with open('accident_severity_model.pkl', 'wb') as f:\n",
    "#         pickle.dump(model_artifacts, f)\n",
    "    \n",
    "#     # Save metadata\n",
    "#     metadata = {\n",
    "#         'model_info': {\n",
    "#             'best_model': best_model_name,\n",
    "#             'performance': results[best_model_name],\n",
    "#             'target_classes': le_target.classes_.tolist(),\n",
    "#             'feature_count': len(numeric_features) + len(ordinal_cols) + len(nominal_cols)\n",
    "#         },\n",
    "#         'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "#         'dataset_info': {\n",
    "#             'total_samples': len(df),\n",
    "#             'training_samples': len(X_train),\n",
    "#             'test_samples': len(X_test)\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     with open('model_metadata.json', 'w') as f:\n",
    "#         json.dump(metadata, f, indent=2)\n",
    "    \n",
    "#     print(\"\\n‚úÖ Model training completed successfully!\")\n",
    "#     print(\"üíæ Model saved as 'accident_severity_model.pkl'\")\n",
    "#     print(\"üìã Metadata saved as 'model_metadata.json'\")\n",
    "    \n",
    "#     # Create sample visualization\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     models_list = list(results.keys())\n",
    "#     f1_scores = [results[model]['F1-Score'] for model in models_list]\n",
    "    \n",
    "#     plt.barh(models_list, f1_scores, color='skyblue')\n",
    "#     plt.xlabel('F1-Score')\n",
    "#     plt.title('Model Performance Comparison')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "#     print(\"üìà Visualization saved as 'model_comparison.png'\")\n",
    "    \n",
    "#     return model_artifacts, metadata\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5e753-da3a-483d-a042-0520b7fbf605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import json\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "# from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler, LabelEncoder\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e856bda-22c8-4ca7-9baa-42e3ecb5b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Optional imports\n",
    "# try:\n",
    "#     from imblearn.over_sampling import SMOTE\n",
    "#     from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "#     IMBLEARN_AVAILABLE = True\n",
    "# except Exception:\n",
    "#     IMBLEARN_AVAILABLE = False\n",
    "\n",
    "# try:\n",
    "#     from xgboost import XGBClassifier\n",
    "#     XGBOOST_AVAILABLE = True\n",
    "# except Exception:\n",
    "#     XGBOOST_AVAILABLE = False\n",
    "\n",
    "# class AccidentSeverityPredictor:\n",
    "#     def __init__(self):\n",
    "#         self.artifacts = {}\n",
    "#         self.metadata = {}\n",
    "        \n",
    "#     def load_and_enhance_data(self, file_path):\n",
    "#         \"\"\"Load data and create enhanced features\"\"\"\n",
    "#         print(\"üìÇ Loading dataset...\")\n",
    "#         df = pd.read_csv(file_path)\n",
    "#         df.columns = df.columns.str.strip()\n",
    "        \n",
    "#         # Clean data - handle 'na' values and whitespace\n",
    "#         df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "#         df = df.replace('na', np.nan)\n",
    "#         df = df.replace('unknown', np.nan)\n",
    "#         df = df.replace('Unknown', np.nan)\n",
    "        \n",
    "#         # Enhanced time features\n",
    "#         if 'Time' in df.columns:\n",
    "#             print(\"‚è∞ Processing time features...\")\n",
    "#             # Handle different time formats\n",
    "#             df['Time_parsed'] = pd.to_datetime(df['Time'], format='%H:%M:%S', errors='coerce')\n",
    "#             mask = df['Time_parsed'].isna()\n",
    "#             df.loc[mask, 'Time_parsed'] = pd.to_datetime(df.loc[mask, 'Time'], format='%H:%M', errors='coerce')\n",
    "            \n",
    "#             df['Hour_of_Day'] = df['Time_parsed'].dt.hour.fillna(12)  # Default to noon if missing\n",
    "#             df['Time_of_Day'] = pd.cut(df['Hour_of_Day'], \n",
    "#                                      bins=[0, 6, 12, 18, 24], \n",
    "#                                      labels=['Night', 'Morning', 'Afternoon', 'Evening'],\n",
    "#                                      include_lowest=True)\n",
    "#             df = df.drop(columns=['Time', 'Time_parsed'], errors='ignore')\n",
    "        \n",
    "#         # Create accident severity risk score\n",
    "#         risk_factors = []\n",
    "#         if 'Number_of_vehicles_involved' in df.columns:\n",
    "#             risk_factors.append(df['Number_of_vehicles_involved'])\n",
    "#         if 'Number_of_casualties' in df.columns:\n",
    "#             risk_factors.append(df['Number_of_casualties'])\n",
    "        \n",
    "#         if risk_factors:\n",
    "#             df['Risk_Score'] = sum(risk_factors) / len(risk_factors)\n",
    "#         else:\n",
    "#             df['Risk_Score'] = 1.0  # Default risk score\n",
    "        \n",
    "#         # Enhanced driver experience categorization\n",
    "#         if 'Driving_experience' in df.columns:\n",
    "#             df['Experience_Level'] = df['Driving_experience'].map({\n",
    "#                 'No Licence': 'Novice',\n",
    "#                 'Below 1yr': 'Novice', \n",
    "#                 '1-2yr': 'Beginner',\n",
    "#                 '2-5yr': 'Intermediate',\n",
    "#                 '5-10yr': 'Experienced',\n",
    "#                 'Above 10yr': 'Expert',\n",
    "#                 'Unknown': 'Unknown',\n",
    "#                 'nan': 'Unknown'\n",
    "#             }).fillna('Unknown')\n",
    "        \n",
    "#         print(f\"‚úÖ Enhanced dataset with new features. Shape: {df.shape}\")\n",
    "#         return df\n",
    "\n",
    "#     def analyze_dataset(self, df, target_col):\n",
    "#         \"\"\"Comprehensive dataset analysis\"\"\"\n",
    "#         print(\"\\nüîç Dataset Analysis:\")\n",
    "#         print(f\"   Total samples: {len(df)}\")\n",
    "#         print(f\"   Total features: {len(df.columns)}\")\n",
    "#         print(f\"   Target distribution:\\n{df[target_col].value_counts()}\")\n",
    "        \n",
    "#         # Missing values analysis\n",
    "#         missing = df.isnull().sum()\n",
    "#         if missing.sum() > 0:\n",
    "#             print(f\"   Missing values: {missing[missing > 0].to_dict()}\")\n",
    "        \n",
    "#         return df\n",
    "\n",
    "#     def create_preprocessor(self, numeric_features, ordinal_cols, nominal_cols, ordinal_categories):\n",
    "#         \"\"\"Create advanced preprocessing pipeline\"\"\"\n",
    "#         print(\"üîß Creating preprocessing pipeline...\")\n",
    "        \n",
    "#         numeric_transformer = Pipeline([\n",
    "#             ('imputer', SimpleImputer(strategy='median')),\n",
    "#             ('scaler', StandardScaler())\n",
    "#         ])\n",
    "\n",
    "#         ordinal_transformer = Pipeline([\n",
    "#             ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#             ('encoder', OrdinalEncoder(categories=ordinal_categories, \n",
    "#                                       handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "#         ])\n",
    "\n",
    "#         nominal_transformer = Pipeline([\n",
    "#             ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#             ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "#         ])\n",
    "\n",
    "#         preprocessor = ColumnTransformer(\n",
    "#             transformers=[\n",
    "#                 ('num', numeric_transformer, numeric_features),\n",
    "#                 ('ord', ordinal_transformer, ordinal_cols),\n",
    "#                 ('cat', nominal_transformer, nominal_cols)\n",
    "#             ],\n",
    "#             remainder='drop'\n",
    "#         )\n",
    "        \n",
    "#         return preprocessor\n",
    "\n",
    "#     def train_models(self, file_path=\"accident-project.csv\"):\n",
    "#         \"\"\"Main training function\"\"\"\n",
    "#         print(\"üöÄ Starting Advanced Model Training...\")\n",
    "        \n",
    "#         # Load and prepare data\n",
    "#         target_col = \"Accident_severity\"\n",
    "#         df = self.load_and_enhance_data(file_path)\n",
    "        \n",
    "#         # Basic cleaning\n",
    "#         df = df.drop_duplicates()\n",
    "#         df = df[~df[target_col].isna()].copy()\n",
    "        \n",
    "#         # Analyze dataset\n",
    "#         df = self.analyze_dataset(df, target_col)\n",
    "        \n",
    "#         # Feature groups - using actual columns from your dataset\n",
    "#         numeric_features = ['Number_of_vehicles_involved', 'Number_of_casualties', 'Hour_of_Day', 'Risk_Score']\n",
    "#         numeric_features = [col for col in numeric_features if col in df.columns]\n",
    "        \n",
    "#         # Define categorical features based on common accident dataset columns\n",
    "#         categorical_features = ['Day_of_week', 'Age_band_of_driver', 'Sex_of_driver', \n",
    "#                                'Educational_level', 'Driving_experience', 'Type_of_vehicle',\n",
    "#                                'Area_accident_occured', 'Road_surface_conditions', \n",
    "#                                'Light_conditions', 'Weather_conditions', 'Time_of_Day',\n",
    "#                                'Experience_Level']\n",
    "#         categorical_features = [col for col in categorical_features if col in df.columns]\n",
    "        \n",
    "#         # Remove target if it sneaks in\n",
    "#         numeric_features = [c for c in numeric_features if c != target_col]\n",
    "#         categorical_features = [c for c in categorical_features if c != target_col]\n",
    "        \n",
    "#         print(f\"üîß Processing {len(numeric_features)} numeric, {len(categorical_features)} categorical features\")\n",
    "        \n",
    "#         # Ordinal features with proper ordering\n",
    "#         ordinal_mappings = {\n",
    "#             'Day_of_week': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "#             'Age_band_of_driver': ['Under 18', '18-30', '31-50', 'Over 51'],\n",
    "#             'Educational_level': ['Illiterate', 'Elementary school', 'Junior high school', \n",
    "#                                  'High school', 'Above high school'],\n",
    "#             'Driving_experience': ['No Licence', 'Below 1yr', '1-2yr', '2-5yr', '5-10yr', 'Above 10yr'],\n",
    "#             'Experience_Level': ['Novice', 'Beginner', 'Intermediate', 'Experienced', 'Expert']\n",
    "#         }\n",
    "        \n",
    "#         # Only use ordinal mappings for columns that exist in dataset\n",
    "#         ordinal_cols = [col for col in ordinal_mappings.keys() if col in df.columns]\n",
    "#         ordinal_categories = [ordinal_mappings[col] for col in ordinal_cols]\n",
    "        \n",
    "#         # Nominal features are the rest\n",
    "#         nominal_cols = [c for c in categorical_features if c not in ordinal_cols]\n",
    "        \n",
    "#         print(f\"   - Ordinal features: {ordinal_cols}\")\n",
    "#         print(f\"   - Nominal features: {nominal_cols}\")\n",
    "        \n",
    "#         # Create preprocessor\n",
    "#         preprocessor = self.create_preprocessor(numeric_features, ordinal_cols, nominal_cols, ordinal_categories)\n",
    "        \n",
    "#         # Prepare target\n",
    "#         le_target = LabelEncoder()\n",
    "#         y = le_target.fit_transform(df[target_col])\n",
    "#         X = df.drop(columns=[target_col])\n",
    "        \n",
    "#         target_mapping = dict(zip(le_target.classes_, le_target.transform(le_target.classes_)))\n",
    "#         print(f\"üéØ Target classes: {target_mapping}\")\n",
    "        \n",
    "#         # Train-test split\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(\n",
    "#             X, y, test_size=0.2, stratify=y, random_state=42\n",
    "#         )\n",
    "        \n",
    "#         print(f\"üìä Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "        \n",
    "#         # Define models with optimized parameters\n",
    "#         models = {\n",
    "#             'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
    "#             'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced', max_depth=10),\n",
    "#             'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced', \n",
    "#                                                   n_estimators=200, max_depth=15),\n",
    "#             'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100, max_depth=6),\n",
    "#             'SVM': SVC(random_state=42, class_weight='balanced', probability=True, kernel='rbf')\n",
    "#         }\n",
    "        \n",
    "#         if XGBOOST_AVAILABLE:\n",
    "#             models['XGBoost'] = XGBClassifier(random_state=42, eval_metric='mlogloss', \n",
    "#                                             n_estimators=200, max_depth=6)\n",
    "        \n",
    "#         # Train and evaluate models\n",
    "#         best_models = {}\n",
    "#         results = {}\n",
    "#         feature_importances = {}\n",
    "        \n",
    "#         print(\"\\nüîÆ Training and Evaluating Models...\")\n",
    "#         for name, model in models.items():\n",
    "#             print(f\"   Training {name}...\")\n",
    "            \n",
    "#             # Create pipeline\n",
    "#             if IMBLEARN_AVAILABLE:\n",
    "#                 pipeline = ImbPipeline([\n",
    "#                     ('preprocessor', preprocessor),\n",
    "#                     ('smote', SMOTE(random_state=42)),\n",
    "#                     ('clf', model)\n",
    "#                 ])\n",
    "#             else:\n",
    "#                 pipeline = Pipeline([\n",
    "#                     ('preprocessor', preprocessor),\n",
    "#                     ('clf', model)\n",
    "#                 ])\n",
    "            \n",
    "#             # Train model\n",
    "#             pipeline.fit(X_train, y_train)\n",
    "            \n",
    "#             # Evaluate\n",
    "#             y_pred = pipeline.predict(X_test)\n",
    "#             y_proba = pipeline.predict_proba(X_test)\n",
    "            \n",
    "#             metrics = {\n",
    "#                 'Accuracy': accuracy_score(y_test, y_pred),\n",
    "#                 'Precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "#                 'Recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "#                 'F1-Score': f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "#             }\n",
    "            \n",
    "#             best_models[name] = pipeline\n",
    "#             results[name] = metrics\n",
    "            \n",
    "#             # Store feature importance if available\n",
    "#             if hasattr(model, 'feature_importances_'):\n",
    "#                 try:\n",
    "#                     # Get feature names after preprocessing\n",
    "#                     feature_names = self.get_feature_names(pipeline.named_steps['preprocessor'])\n",
    "#                     importances = model.feature_importances_\n",
    "#                     feature_importances[name] = {\n",
    "#                         'features': feature_names,\n",
    "#                         'importances': importances\n",
    "#                     }\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"      Could not extract feature importance for {name}: {e}\")\n",
    "            \n",
    "#             print(f\"      ‚úÖ {name}: F1 = {metrics['F1-Score']:.4f}, Accuracy = {metrics['Accuracy']:.4f}\")\n",
    "        \n",
    "#         # Select best model\n",
    "#         best_model_name = max(results, key=lambda x: results[x]['F1-Score'])\n",
    "#         best_model = best_models[best_model_name]\n",
    "        \n",
    "#         print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "#         print(f\"üìä Performance: {results[best_model_name]}\")\n",
    "        \n",
    "#         # Generate comprehensive report\n",
    "#         self.generate_model_report(best_model, X_test, y_test, le_target, best_model_name)\n",
    "        \n",
    "#         # Save model artifacts\n",
    "#         self.save_model_artifacts(best_models, results, preprocessor, le_target, target_mapping,\n",
    "#                                  numeric_features, ordinal_cols, nominal_cols, best_model_name,\n",
    "#                                  feature_importances, df, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "#         return best_models[best_model_name]\n",
    "\n",
    "#     def get_feature_names(self, column_transformer):\n",
    "#         \"\"\"Get feature names after preprocessing\"\"\"\n",
    "#         feature_names = []\n",
    "        \n",
    "#         # Numeric features\n",
    "#         for name, transformer, features in column_transformer.transformers_:\n",
    "#             if name == 'num':\n",
    "#                 feature_names.extend(features)\n",
    "#             elif name == 'ord':\n",
    "#                 feature_names.extend(features)\n",
    "#             elif name == 'cat':\n",
    "#                 if hasattr(transformer.named_steps['encoder'], 'get_feature_names_out'):\n",
    "#                     cat_features = transformer.named_steps['encoder'].get_feature_names_out(features)\n",
    "#                     feature_names.extend(cat_features)\n",
    "#                 else:\n",
    "#                     feature_names.extend(features)\n",
    "        \n",
    "#         return feature_names\n",
    "\n",
    "#     def generate_model_report(self, model, X_test, y_test, le_target, model_name):\n",
    "#         \"\"\"Generate comprehensive model performance report\"\"\"\n",
    "#         print(\"\\nüìã Generating Model Performance Report...\")\n",
    "        \n",
    "#         y_pred = model.predict(X_test)\n",
    "#         y_proba = model.predict_proba(X_test)\n",
    "        \n",
    "#         # Classification report\n",
    "#         print(\"\\nüìä Classification Report:\")\n",
    "#         print(classification_report(y_test, y_pred, target_names=le_target.classes_))\n",
    "        \n",
    "#         # Confusion matrix\n",
    "#         cm = confusion_matrix(y_test, y_pred)\n",
    "#         print(f\"\\nüéØ Confusion Matrix:\\n{cm}\")\n",
    "        \n",
    "#         # Create visualization\n",
    "#         self.create_performance_visualizations(cm, le_target.classes_, model_name)\n",
    "\n",
    "#     def create_performance_visualizations(self, cm, class_names, model_name):\n",
    "#         \"\"\"Create performance visualizations\"\"\"\n",
    "#         try:\n",
    "#             # Confusion matrix heatmap\n",
    "#             plt.figure(figsize=(8, 6))\n",
    "#             sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "#                        xticklabels=class_names, yticklabels=class_names)\n",
    "#             plt.title(f'Confusion Matrix - {model_name}')\n",
    "#             plt.xlabel('Predicted')\n",
    "#             plt.ylabel('Actual')\n",
    "#             plt.tight_layout()\n",
    "#             plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "#             plt.close()\n",
    "            \n",
    "#             print(\"‚úÖ Performance visualizations saved\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ö†Ô∏è  Could not create visualizations: {e}\")\n",
    "\n",
    "#     def save_model_artifacts(self, best_models, results, preprocessor, le_target, target_mapping,\n",
    "#                            numeric_features, ordinal_cols, nominal_cols, best_model_name,\n",
    "#                            feature_importances, df, X_train, X_test, y_train, y_test):\n",
    "#         \"\"\"Save all model artifacts\"\"\"\n",
    "#         print(\"\\nüíæ Saving model artifacts...\")\n",
    "        \n",
    "#         best_model = best_models[best_model_name]\n",
    "        \n",
    "#         model_artifacts = {\n",
    "#             'model': best_model,\n",
    "#             'all_models': best_models,\n",
    "#             'preprocessor': preprocessor,\n",
    "#             'label_encoder': le_target,\n",
    "#             'target_mapping': target_mapping,\n",
    "#             'feature_names': {\n",
    "#                 'numeric': numeric_features,\n",
    "#                 'ordinal': ordinal_cols,\n",
    "#                 'nominal': nominal_cols\n",
    "#             },\n",
    "#             'model_performance': results,\n",
    "#             'best_model_name': best_model_name,\n",
    "#             'feature_importances': feature_importances,\n",
    "#             'dataset_info': {\n",
    "#                 'train_shape': X_train.shape,\n",
    "#                 'test_shape': X_test.shape,\n",
    "#                 'target_distribution': dict(df['Accident_severity'].value_counts())\n",
    "#             }\n",
    "#         }\n",
    "        \n",
    "#         # Save with pickle\n",
    "#         with open('accident_severity_model.pkl', 'wb') as f:\n",
    "#             pickle.dump(model_artifacts, f)\n",
    "        \n",
    "#         # Save metadata\n",
    "#         metadata = {\n",
    "#             'model_info': {\n",
    "#                 'best_model': best_model_name,\n",
    "#                 'performance': results[best_model_name],\n",
    "#                 'target_classes': le_target.classes_.tolist(),\n",
    "#                 'feature_count': len(numeric_features) + len(ordinal_cols) + len(nominal_cols),\n",
    "#                 'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "#             },\n",
    "#             'dataset_info': {\n",
    "#                 'total_samples': len(df),\n",
    "#                 'training_samples': len(X_train),\n",
    "#                 'test_samples': len(X_test),\n",
    "#                 'feature_breakdown': {\n",
    "#                     'numeric': len(numeric_features),\n",
    "#                     'ordinal': len(ordinal_cols),\n",
    "#                     'nominal': len(nominal_cols)\n",
    "#                 }\n",
    "#             },\n",
    "#             'model_comparison': results\n",
    "#         }\n",
    "        \n",
    "#         with open('model_metadata.json', 'w') as f:\n",
    "#             json.dump(metadata, f, indent=2)\n",
    "        \n",
    "#         print(\"‚úÖ Model artifacts saved successfully!\")\n",
    "#         print(f\"üìÅ Files created:\")\n",
    "#         print(f\"   - accident_severity_model.pkl\")\n",
    "#         print(f\"   - model_metadata.json\") \n",
    "#         print(f\"   - confusion_matrix.png\")\n",
    "        \n",
    "#         # Print final summary\n",
    "#         print(f\"\\nüéâ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "#         print(f\"üèÜ Best Model: {best_model_name}\")\n",
    "#         print(f\"üìà Best F1-Score: {results[best_model_name]['F1-Score']:.4f}\")\n",
    "#         print(f\"üéØ Best Accuracy: {results[best_model_name]['Accuracy']:.4f}\")\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"Main execution function\"\"\"\n",
    "#     try:\n",
    "#         predictor = AccidentSeverityPredictor()\n",
    "#         model = predictor.train_models(\"accident-project.csv\")\n",
    "        \n",
    "#         print(\"\\nüöÄ Next steps:\")\n",
    "#         print(\"   1. Run: streamlit run enhanced_streamlit_app.py\")\n",
    "#         print(\"   2. Open http://localhost:8501 in your browser\")\n",
    "#         print(\"   3. Start making predictions!\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error during training: {e}\")\n",
    "#         raise\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2485f47-68ae-4dec-aa8b-b94a18ceffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_training_fixed.py\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"üöÄ Starting Model Training with Streamlit-Compatible Features...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e4382-cd62-4720-8b6f-eb1bc8cc2d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "try:\n",
    "    df = pd.read_csv(\"accident-project.csv\")\n",
    "    print(f\"‚úÖ Data loaded successfully! Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå File 'accident-project.csv' not found!\")\n",
    "    exit()\n",
    "\n",
    "# Basic cleaning\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df = df.replace('na', np.nan)\n",
    "df = df.replace('unknown', np.nan)\n",
    "\n",
    "# Handle missing target\n",
    "target_col = \"Accident_severity\"\n",
    "if target_col not in df.columns:\n",
    "    print(f\"‚ùå Target column '{target_col}' not found!\")\n",
    "    exit()\n",
    "\n",
    "df = df[~df[target_col].isna()].copy()\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(f\"üìä Dataset after cleaning: {df.shape}\")\n",
    "print(f\"üéØ Target distribution:\\n{df[target_col].value_counts()}\")\n",
    "\n",
    "# Define ONLY the features we'll use in Streamlit (matching your sidebar inputs)\n",
    "streamlit_features = [\n",
    "    'Hour_of_Day', 'Time_of_Day', 'Age_band_of_driver', 'Sex_of_driver',\n",
    "    'Educational_level', 'Driving_experience', 'Type_of_vehicle',\n",
    "    'Area_accident_occured', 'Road_surface_conditions', 'Light_conditions',\n",
    "    'Weather_conditions', 'Number_of_vehicles_involved', 'Number_of_casualties',\n",
    "    'Risk_Score', 'Day_of_week', 'Service_year_of_vehicle', 'Experience_Level'\n",
    "]\n",
    "\n",
    "# Feature engineering to create the features we need\n",
    "if 'Time' in df.columns:\n",
    "    try:\n",
    "        df['Hour_of_Day'] = pd.to_datetime(df['Time'], errors='coerce').dt.hour.fillna(12)\n",
    "        df['Time_of_Day'] = pd.cut(df['Hour_of_Day'], \n",
    "                                 bins=[0, 6, 12, 18, 24], \n",
    "                                 labels=['Night', 'Morning', 'Afternoon', 'Evening'])\n",
    "    except:\n",
    "        df['Hour_of_Day'] = 12\n",
    "        df['Time_of_Day'] = 'Afternoon'\n",
    "\n",
    "# Create Risk_Score\n",
    "if 'Number_of_vehicles_involved' in df.columns and 'Number_of_casualties' in df.columns:\n",
    "    df['Risk_Score'] = (df['Number_of_vehicles_involved'] + df['Number_of_casualties']) / 2\n",
    "else:\n",
    "    df['Risk_Score'] = 1.0\n",
    "\n",
    "# Create Experience_Level\n",
    "if 'Driving_experience' in df.columns:\n",
    "    df['Experience_Level'] = df['Driving_experience'].map({\n",
    "        'No Licence': 'Novice',\n",
    "        'Below 1yr': 'Novice', \n",
    "        '1-2yr': 'Beginner',\n",
    "        '2-5yr': 'Intermediate',\n",
    "        '5-10yr': 'Experienced',\n",
    "        'Above 10yr': 'Expert',\n",
    "        'Unknown': 'Intermediate'\n",
    "    }).fillna('Intermediate')\n",
    "\n",
    "# Fill missing values for essential features\n",
    "default_values = {\n",
    "    'Day_of_week': 'Monday',\n",
    "    'Service_year_of_vehicle': '5-10yr',\n",
    "    'Age_band_of_driver': '18-30',\n",
    "    'Sex_of_driver': 'Male',\n",
    "    'Educational_level': 'High school',\n",
    "    'Driving_experience': '2-5yr',\n",
    "    'Type_of_vehicle': 'Automobile',\n",
    "    'Area_accident_occured': 'Residential areas',\n",
    "    'Road_surface_conditions': 'Dry',\n",
    "    'Light_conditions': 'Daylight',\n",
    "    'Weather_conditions': 'Normal',\n",
    "    'Number_of_vehicles_involved': 2,\n",
    "    'Number_of_casualties': 1\n",
    "}\n",
    "\n",
    "for col, default_val in default_values.items():\n",
    "    if col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].fillna(default_val)\n",
    "        else:\n",
    "            df[col] = df[col].fillna(default_val)\n",
    "\n",
    "# Select only the features we'll use in Streamlit\n",
    "available_features = [f for f in streamlit_features if f in df.columns]\n",
    "print(f\"üîß Using features: {available_features}\")\n",
    "\n",
    "# Define feature types\n",
    "numeric_features = ['Hour_of_Day', 'Number_of_vehicles_involved', 'Number_of_casualties', 'Risk_Score']\n",
    "numeric_features = [f for f in numeric_features if f in available_features]\n",
    "\n",
    "categorical_features = [f for f in available_features if f not in numeric_features and f != target_col]\n",
    "\n",
    "# Define ordinal features\n",
    "ordinal_mappings = {\n",
    "    'Day_of_week': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "    'Age_band_of_driver': ['Under 18', '18-30', '31-50', 'Over 51'],\n",
    "    'Educational_level': ['Elementary school', 'Junior high school', 'High school', 'Above high school'],\n",
    "    'Driving_experience': ['Below 1yr', '1-2yr', '2-5yr', '5-10yr', 'Above 10yr'],\n",
    "    'Service_year_of_vehicle': ['Below 1yr', '1-2yr', '2-5yr', '5-10yr', 'Above 10yr'],\n",
    "    'Experience_Level': ['Novice', 'Beginner', 'Intermediate', 'Experienced', 'Expert'],\n",
    "    'Time_of_Day': ['Night', 'Morning', 'Afternoon', 'Evening']\n",
    "}\n",
    "\n",
    "ordinal_cols = [col for col in ordinal_mappings.keys() if col in categorical_features]\n",
    "ordinal_categories = [ordinal_mappings[col] for col in ordinal_cols]\n",
    "nominal_cols = [col for col in categorical_features if col not in ordinal_cols]\n",
    "\n",
    "print(f\"üìä Numeric features: {numeric_features}\")\n",
    "print(f\"üìä Ordinal features: {ordinal_cols}\")\n",
    "print(f\"üìä Nominal features: {nominal_cols}\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "ordinal_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(categories=ordinal_categories, \n",
    "                              handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "nominal_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('ord', ordinal_transformer, ordinal_cols),\n",
    "        ('cat', nominal_transformer, nominal_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Prepare target\n",
    "le_target = LabelEncoder()\n",
    "y = le_target.fit_transform(df[target_col])\n",
    "X = df[available_features]  # Use only the features we'll have in Streamlit\n",
    "\n",
    "target_mapping = dict(zip(le_target.classes_, le_target.transform(le_target.classes_)))\n",
    "print(f\"üéØ Target classes: {target_mapping}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüîÆ Training Random Forest Model...\")\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Train\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "    'Recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "    'F1-Score': f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "}\n",
    "\n",
    "print(f\"üìä Model Performance:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "# Save model artifacts\n",
    "model_artifacts = {\n",
    "    'model': pipeline,\n",
    "    'preprocessor': preprocessor,\n",
    "    'label_encoder': le_target,\n",
    "    'target_mapping': target_mapping,\n",
    "    'feature_names': available_features,  # Store the exact features we used\n",
    "    'model_performance': {'Random Forest': metrics},\n",
    "    'best_model_name': 'Random Forest',\n",
    "    'required_features': available_features  # Explicitly store required features\n",
    "}\n",
    "\n",
    "with open('accident_severity_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_artifacts, f)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_info': {\n",
    "        'best_model': 'Random Forest',\n",
    "        'performance': metrics,\n",
    "        'target_classes': le_target.classes_.tolist(),\n",
    "        'feature_count': len(available_features),\n",
    "        'required_features': available_features\n",
    "    },\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset_info': {\n",
    "        'total_samples': len(df),\n",
    "        'training_samples': len(X_train),\n",
    "        'test_samples': len(X_test)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('model_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Model training completed successfully!\")\n",
    "print(f\"üíæ Model saved with {len(available_features)} features\")\n",
    "print(\"üöÄ Next: Run 'streamlit run streamlit_app.py'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec55b1b-3e42-4890-a78d-a2a9e96fd675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
